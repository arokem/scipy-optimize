<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="pandoc">
    <title>Model evaulation with cross-validation</title>
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" type="text/css" href="css/bootstrap/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="css/bootstrap/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="css/swc.css" />
    <meta charset="UTF-8" />
    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body class="lesson">
    <div class="container card">
      <div class="banner">
      
      </div>
      <article>
      <div class="row">
        <div class="col-md-10 col-md-offset-1">
                    <h1 class="title">Model evaulation with cross-validation</h1>
          <h1 id="model-evaluation-with-cross-validation">Model evaluation with cross-validation</h1>
<section class="objectives panel panel-warning">
<div class="panel-heading">
<h2 id="learning-objectives"><span class="glyphicon glyphicon-certificate"></span>Learning Objectives</h2>
</div>
<div class="panel-body">
<ul>
<li>Learners can define and identify overfitting.</li>
<li>Learners can use split-half cross-validation to evaluate model error.</li>
</ul>
</div>
</section>
<p>How do we know that a model is <em>good enough</em>?</p>
<p>In the previous section, we managed to reduce the SSE substantially relative to the linear model, by fitting a non-linear, but it’s still not perfect. How about trying a more complicated model, with more parameters? Another function that is often used to fit psychometric data is the Weibull cumulative distribution function, named after the great Swedish mathematician and engineer <a href="http://en.wikipedia.org/wiki/Waloddi_Weibull">Waloddi Weibull</a></p>
<pre class="sourceCode python"><code class="sourceCode python">
<span class="kw">def</span> weibull(x, threshx, slope, lower_asymp, upper_asymp):
    <span class="co">&quot;&quot;&quot;</span>
<span class="co">    The Weibull cumulative distribution function</span>

<span class="co">    Parameters</span>
<span class="co">    ----------</span>
<span class="co">    x : float or array</span>
<span class="co">       The values of x over which to evaluate the cumulative Weibull function</span>

<span class="co">    threshx : float</span>
<span class="co">       The value of x at the deflection point of the function.</span>
<span class="co">       For a lower_asymp set to 0.5, this is at approximately y=0.81  </span>

<span class="co">    slope : float</span>
<span class="co">        The slope of the function at the deflection point.</span>

<span class="co">    lower_asymp : float</span>
<span class="co">        The lower asymptote of the function</span>

<span class="co">    upper_asymp : float</span>
<span class="co">        The upper asymptote of the function.</span>
<span class="co">    &quot;&quot;&quot;</span>
    threshy = <span class="dv">1</span> - (lower_asymp * np.exp(-<span class="dv">1</span>))
    k = (-np.log((<span class="dv">1</span> - threshy) / (lower_asymp))) ** (<span class="dv">1</span> / slope)
    m = (lower_asymp + upper_asymp - <span class="dv">1</span>) * np.exp(-(k * x / threshx) ** slope)
    <span class="kw">return</span> upper_asymp - m</code></pre>
<p>As you can see, this function has several more parameters to account for features of the data that may interest us.</p>
<p>Let’s see what that might look like:</p>
<pre class="sourceCode python"><code class="sourceCode python">
fig, ax = plt.subplots(<span class="dv">1</span>)
ax.plot(x, weibull(x, <span class="fl">0.5</span>, <span class="fl">3.5</span>, <span class="fl">0.5</span>, <span class="fl">0.95</span>), label=<span class="st">&#39;threshx=0.5, slope=3.5, lower_asymp=0.5, upper_asymp=0.95&#39;</span>)
ax.plot(x, weibull(x, <span class="fl">0.5</span>, <span class="fl">3.5</span>, <span class="dv">1</span>, <span class="fl">0.95</span>),  label=<span class="st">&#39;threshx=0.5, slope=3.5, lower_asymp=1, upper_asymp=0.95&#39;</span>)
ax.plot(x, weibull(x, <span class="fl">0.5</span>, <span class="fl">3.5</span>, <span class="dv">1</span>, <span class="fl">0.85</span>), label=<span class="st">&#39;threshx=0.5, slope=3.5, lower_asymp=1, upper_asymp=0.85&#39;</span>)
ax.plot(x, weibull(x, <span class="fl">0.25</span>, <span class="fl">3.5</span>, <span class="fl">0.5</span>, <span class="fl">0.95</span>), label=<span class="st">&#39;threshx=0.25, slope=3.5, lower_asymp=0.5, upper_asymp=0.95&#39;</span>)
ax.plot(x, weibull(x, <span class="fl">0.75</span>, <span class="fl">3.5</span>, <span class="dv">1</span>, <span class="fl">0.85</span>), label=<span class="st">&#39;threshx=0.75, slope=3.5, lower_asymp=1, upper_asymp=0.85&#39;</span>)
ax.grid(<span class="st">&#39;on&#39;</span>)
fig.set_size_inches([<span class="dv">14</span>, <span class="dv">8</span>])
plt.legend(loc=<span class="st">&#39;lower right&#39;</span>)</code></pre>
<div class="figure">
<img src="img/figure7.png" alt="Weibull functions" />
<p class="caption">Weibull functions</p>
</div>
<p>We go ahead and fit this function as well, using <code>curve_fit</code></p>
<pre class="sourceCode python"><code class="sourceCode python">
params_ortho, cov_ortho = opt.curve_fit(weibull, x_ortho, y_ortho)
params_para, cov_para = opt.curve_fit(weibull, x_para, y_para)</code></pre>
<p>And examine the results:</p>
<pre class="sourceCode python"><code class="sourceCode python">
y_ortho_hat = weibull(x, params_ortho[<span class="dv">0</span>], params_ortho[<span class="dv">1</span>], params_ortho[<span class="dv">2</span>], params_ortho[<span class="dv">3</span>])
fig, ax = plt.subplots(<span class="dv">1</span>)
ax.plot(x, y_ortho_hat)
ax.plot(x_ortho, y_ortho, <span class="st">&#39;bo&#39;</span>)

y_para_hat = weibull(x, params_para[<span class="dv">0</span>], params_para[<span class="dv">1</span>], params_para[<span class="dv">2</span>], params_para[<span class="dv">3</span>])
ax.plot(x, y_para_hat)
ax.plot(x_para, y_para, <span class="st">&#39;go&#39;</span>)

ax.grid(<span class="st">&quot;on&quot;</span>)
fig.set_size_inches([<span class="dv">8</span>, <span class="dv">8</span>])</code></pre>
<div class="figure">
<img src="img/figure8.png" alt="Weibull fits" />
<p class="caption">Weibull fits</p>
</div>
<p>Calculating the error, we find that the SSE is smaller for this model:</p>
<pre class="sourceCode python"><code class="sourceCode python">
SSE_para = np.<span class="dt">sum</span>((weibull(x_para, params_para[<span class="dv">0</span>], params_para[<span class="dv">1</span>], params_para[<span class="dv">2</span>], params_para[<span class="dv">3</span>]) - y_para) ** <span class="dv">2</span>)
SSE_ortho = np.<span class="dt">sum</span>((weibull(x_ortho, params_ortho[<span class="dv">0</span>], params_ortho[<span class="dv">1</span>], params_ortho[<span class="dv">2</span>], params_ortho[<span class="dv">3</span>]) - y_ortho) ** <span class="dv">2</span>)</code></pre>
<p>Should we switch over to this model? It seems to be doing better in fitting the data.</p>
<h2 id="overfitting">Overfitting</h2>
<p>One of the reasons we should be suspicious of the more accurate second model is tha the Weibull function has more parameters. This is sometimes referred to as the “degrees of freedom” of the model, and intuitively just means that the more parameters a function has, the more adjustments you can make to the estimated y values that you could consistently get across a range of x values. Let’s examine the most extreme case of that. Consider the case of fitting a polynomial of degree 6 to these data (7 parameters, including <span class="math">\(\beta_0\)</span>):</p>
<pre class="sourceCode python"><code class="sourceCode python">
beta_ortho = np.polyfit(x_ortho, y_ortho, <span class="dv">6</span>)
beta_para = np.polyfit(x_para, y_para, <span class="dv">6</span>)</code></pre>
<p>This model has even smaller error on the data:</p>
<pre class="sourceCode python"><code class="sourceCode python">
SE_ortho = np.<span class="dt">sum</span>((y_ortho - np.polyval(beta_ortho, x_ortho))**<span class="dv">2</span>)
SSE_para = np.<span class="dt">sum</span>((y_para - np.polyval(beta_para, x_para))**<span class="dv">2</span>)</code></pre>
<p>But examining the curves of the fits, you can see that it does some absurd things in order to fit the data:</p>
<div class="figure">
<img src="img/figure9.png" alt="Fits of polynomials with degree 6" />
<p class="caption">Fits of polynomials with degree 6</p>
</div>
<p>That is, it bends to match not only the large-scale trends in the data, but also the noise associated with each data point. This match to the noise that characterizes the sample is called “overfitting”.</p>
<p>Overfitting generally becomes worse as a model becomes more flexible. This can roughly be quantified by the number of parameters in the model. That is, a polynomial of degree 6 will always fit the data more accurately than a polynomial of degree 5, if only because of overfitting to the noise in the sample.</p>
<aside class="callout panel panel-info">
<div class="panel-heading">
<h2 id="the-bias-variance-tradeoff"><span class="glyphicon glyphicon-pushpin"></span>The bias-variance tradeoff</h2>
</div>
<div class="panel-body">
<p>The tradeoff between different levels of model complexity and different sources of error is known as the bias-variance tradeff. We will not go into more detailed explanations of this tradeoff here, but Scott Fortmann-Roe wrote an <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">essay</a> that elegantly demonstrates the concepts invloved, and offers additional reading.</p>
</div>
</aside>
<aside class="callout panel panel-info">
<div class="panel-heading">
<h2 id="formalparamteric-model-selection"><span class="glyphicon glyphicon-pushpin"></span>Formal/paramteric model selection</h2>
</div>
<div class="panel-body">
<p>An F-test can be used to compare models that are <em>nested</em>. That is, a model in which an additional set of (one or more) parameters is added to an existing model. In addition, a few metrics have been developed to compare different models (also models that are not nested) based on the SSE and the number of parameters. <a href="http://en.wikipedia.org/wiki/Bayesian_information_criterion">These</a> <a href="http://en.wikipedia.org/wiki/Akaike_information_criterion">metrics</a> generally penalize a model for having many parameters and reward it for getting small SSE. However, these methods generally have some underlying assumptions and are not that transparent to understand. For a nice exposition on how to use these methods see <a href="">this paper</a> (excuse the paywall…). Also, if you are interested in more on this topic, [This discussion] (http://stats.stackexchange.com/questions/20729/best-approach-for-model-selection-bayesian-or-cross-validation) on the stats stack exchange website is an excellent resource.</p>
</div>
</aside>
<h2 id="overcoming-overfitting-model-selection-with-cross-validation">Overcoming overfitting – model selection with cross-validation</h2>
<p>One strategy to overcome overfitting is by separating the noise in the data used to fit the model from the noise in the data used to evaluate the model. This is called “cross-validation”. We fit the model to one sample, and then evaluate it on another. If we haven’t collected two separate samples, it might still be safe to assume that the noise in every individual observation (in the case of these experiments, each trial) is independent, and we can generate two samples by splitting the data up into sub-samples. There are different ways to split up the data, but in this case it makes sense to separately look at odd trials and at even trials, minimizing serial order effects (such as how tired the person got).</p>
<p>We refer to the sample used for fitting the model as the “training set”. Using the training set, we find the parameters of the model</p>
<pre class="sourceCode python"><code class="sourceCode python">
ortho_1 = ortho[::<span class="dv">2</span>]
x_ortho_1, y_ortho_1, n_ortho_1 = transform_data(ortho_train)
params_ortho_1, cov_ortho_1 = opt.curve_fit(cumgauss, x_ortho_1, y_ortho_1)</code></pre>
<p>The left-out sample is called the “testing set”. We use this sample to calculate the error:</p>
<pre class="sourceCode python"><code class="sourceCode python">
ortho_2 = ortho[<span class="dv">1</span>::<span class="dv">2</span>]
x_ortho_2, y_ortho_2, n_ortho_2 = transform_data(ortho_train)
SSE_ortho_2 = np.<span class="dt">sum</span>((cumgauss(x_ortho_2, params_ortho_1[<span class="dv">0</span>], params_ortho_1[<span class="dv">1</span>]) - y_ortho_2) ** <span class="dv">2</span>)</code></pre>
<p>In order for this to be cross-validation, we would want to also do it the other way around: use set #2 as the training set and evaluate on the set #1 as the testing set:</p>
<pre class="sourceCode python"><code class="sourceCode python">
params_ortho_2, cov_ortho_2 = opt.curve_fit(cumgauss, x_ortho_2, y_ortho_2)
SSE_ortho_1 = np.<span class="dt">sum</span>((cumgauss(x_ortho_1, params_ortho_2[<span class="dv">0</span>], params_ortho_2[<span class="dv">1</span>]) - y_ortho_1) ** <span class="dv">2</span>)</code></pre>
<p>To evaluate model over all</p>
<aside class="callout panel panel-info">
<div class="panel-heading">
<h2 id="k-fold-cross-validation"><span class="glyphicon glyphicon-pushpin"></span>K-fold cross-validation</h2>
</div>
<div class="panel-body">
<p>This type of cross-validation is a special case of what we call “k-fold” cross-validation. In each iteration of this procedure <span class="math">\(\frac{1/k}\)</span> of the data is left out for evaulation and the remaining data is used for training. In this case, we are using 2-fold cross-validation, or what is sometimes called &gt; “split-half cross-validation”</p>
</div>
</aside>
<section class="challenge panel panel-success">
<div class="panel-heading">
<h2 id="cross-validation"><span class="glyphicon glyphicon-pencil"></span>Cross-validation</h2>
</div>
<div class="panel-body">
<ol style="list-style-type: decimal">
<li>Write the code to cross-validate the parallel condition (bonus points if you write a function that can do both without changes!)</li>
<li>Evaluate the Weibull model as well. Which model do you think is better?</li>
</ol>
</div>
</section>
        </div>
      </div>
      </article>
      <div class="footer">
        <a class="label swc-blue-bg" href="https://github.com/arokem/scipy-optimize">Source</a>
        <a class="label swc-blue-bg" href="mailto:arokem@gmail.com">Contact</a>
        <a class="label swc-blue-bg" href="LICENSE.html">License</a>
      </div>
    </div>
    <!-- Javascript placed at the end of the document so the pages load faster -->
    <script src="http://software-carpentry.org/v5/js/jquery-1.9.1.min.js"></script>
    <script src="css/bootstrap/bootstrap-js/bootstrap.js"></script>
    <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
  </body>
</html>
